{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ee2a3-fff5-4d5a-bd63-3a9544bc288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?\n",
    "\n",
    "Underfitting: -\n",
    "              A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the \n",
    "data, i.e., it only performs well on training data but performs poorly on testing data. (It’s just like trying to fit undersized pants!) \n",
    "Underfitting destroys the accuracy of our machine learning model.\n",
    "\n",
    "overfitting - \n",
    "               Overfitting: A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. \n",
    "    When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing\n",
    "    with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609ef7f-1f97-4cb4-ab76-cf20502cdec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "Remove layers / number of units per layer (model)\n",
    "As mentioned in L1 or L2 regularization, an over-complex model may more likely overfit. Therefore, we can directly reduce the model’s complexity by\n",
    "removing layers and reduce the size of our model. We may further reduce complexity by decreasing the number of neurons in the fully-connected layers.\n",
    "We should have a model with a complexity that sufficiently balances between underfitting and overfitting for our task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ca0d8-243b-4c74-a6a4-783eeb52745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "\n",
    "\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables \n",
    "accurately, generating a high error rate on both the training set and unseen data. It occurs when a model is too simple, which can be a result of \n",
    "a model needing more training time, more input features, or less regularization. Like overfitting, when a model is underfitted, it cannot establish\n",
    "the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize well to new data,\n",
    "then it cannot be leveraged for classification or prediction tasks. Generalization of a model to new data is ultimately what allows us to use machine \n",
    "learning algorithms every day to make predictions and classify data.\n",
    "\n",
    "High bias and low variance are good indicators of underfitting. Since this behavior can be seen while using the training dataset, underfitted models\n",
    "are usually easier to identify than overfitted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c322e-957a-49f2-a3e0-ec337a68547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "\n",
    "In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated \n",
    "across samples can be reduced by increasing the bias in the estimated parameters.\n",
    "\n",
    " bias and variance\n",
    "\n",
    "Data scientists building machine learning algorithms are forced to make decisions about the level of bias and variance in their models.\n",
    "Ultimately, the trade-off is well known: increasing bias decreases variance, and increasing variance decreases bias. Data scientists have\n",
    "to find the correct balance.When building a supervised machine-learning algorithm, the goal is to achieve low bias and variance for the\n",
    "most accurate predictions. Data scientists must do this while keeping underfitting and overfittingExternal link:open_in_new in mind. A\n",
    "model that exhibits small variance and high bias will underfit the target, while a model with high variance and little bias will overfit\n",
    "the target.A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise\n",
    "unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks \n",
    "regularities in the data.The trade-off challenge depends on the type of model under consideration. A linear machine-learning algorithm will\n",
    "exhibit high bias but low variance. On the other hand, a non-linear algorithm will exhibit low bias but high variance. Using a linear model\n",
    "with a data set that is non-linear will introduce bias into the model. The model will underfit the target functions compared to the training\n",
    "data set. The reverse is true as well — if you use a non-linear model on a linear dataset, the non-linear model will overfit the target \n",
    "function.To deal with these trade-off challenges, a data scientist must build a learning algorithm flexible enough to correctly fit the\n",
    "data. However, if the algorithm has too much flexibility built in, it may be too linear and provide results with a high variance from each\n",
    "training data set.In characterizing the bias-variance trade-off, a data scientist will use standard machine learning metrics, such as \n",
    "training error and test errorExternal link:open_in_new, to determine the accuracy of the model. The Mean Square Error (MSE) can be used in\n",
    "a linear regression model with the training set to train the model with a large portion of the available data and act as a test set to\n",
    "analyze the accuracy of the model with a smaller sample of the data. A small portion of data can be reserved for a final test to assess the\n",
    "errors in the model after the model is selected.There is always tension between bias and variance. In fact, it’s difficult to create a\n",
    "model that has both low bias and variance. The goal is a model that reflects the linearity of the training data but will also be sensitive \n",
    "to unseen data used for predictions or estimates. Data scientists must understand the difference between bias and variance so they can make\n",
    "the necessary compromises to build a model with acceptably accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebbdb40-d28e-4165-8f3f-df522b415f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "\n",
    "We can understand overfitting better by looking at the opposite problem, underfitting.\n",
    "\n",
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in\n",
    "learning from the dataset.Simple learners tend to have less variance in their predictions but more bias towards wrong outcomes. On\n",
    "the other hand, complex learners tend to have more variance in their predictions.Let me give you an analogy to explain overfitting and \n",
    "underfitting.Overfitted models are like subject matter experts:They know a lot about a particular field for example subject matter experts.\n",
    "You ask them anything about the functionality of their tool(even in details), they’ll probably be able to answer you and that too pretty \n",
    "precisely.But when you ask them why the oil price fluctuate, they’ll probably make an informed guess and say something peculiar.\n",
    "In terms of machine learning, we can state them as too much focus on the training set (programmers) and learns complex relations which may \n",
    "not be valid in general for new data (test set).Underfitted models are like those Engineers who wanted to be cricketers but forced by their\n",
    "parents to take up engineering. They will neither know engineering nor cricket pretty well. They never had their heart in what they did and \n",
    "have insufficient knowledge of everything.In terms of machine learning, we can state them as too little focus on the training set.\n",
    "Neither good for training not testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313feb89-027c-4241-a5e7-310c7b092eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "\n",
    "\n",
    "The term \"variance\" refers to the degree of change that may be expected in the estimation of the target function as a result of using \n",
    "multiple sets of training data. The disparity between the values that were predicted and the values that were actually observed is referred \n",
    "to as bias\n",
    "\n",
    "variance- \n",
    "           other words, variance describes how much a random variable differs from its expected value. Variance is based on a single \n",
    "    training set. Variance measures the inconsistency of different predictions using different training sets — it’s not a measure of \n",
    "    overall accuracy.Variance can lead to overfitting, in which small fluctuations in the training set are magnified. A model with high-\n",
    "    level variance may reflect random noise in the training data set instead of the target function. The model should be able to identify \n",
    "    the underlying connections between the input data and variables of the output.A model with low variance means sampled data is close to\n",
    "    where the model predicted it would be. A model with high variance will result in significant changes to the projections of the target\n",
    "    function.Machine learning algorithms with low variance include linear regression, logistics regression, and linear discriminant \n",
    "    analysis. Those with high variance include decision trees, support vector machines and k-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b68d6c-4181-406a-8710-ef0062feb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting?Describe\n",
    "some common regularization techniques and how they work.\n",
    "\n",
    "regularization-\n",
    "                In short, Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes,\n",
    "    or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, \n",
    "    avoiding the risk of Overfitting.\n",
    "\n",
    "Regularization to Prevent Overfitting -\n",
    "                                      Overfitting impacts the accuracy of Machine Learning models. The model attempts to capture the data points that\n",
    "do not represent the accurate properties of data. These data points may be considered as noise. To avoid the occurrence of overfitting, we may \n",
    "use a method called regularization.\n",
    "\n",
    "\n",
    "\n",
    "In this article, we will mathematically explore two types of regularization.\n",
    "\n",
    "Contents\n",
    "(1) Regularization and Overfitting\n",
    "(2) Lasso Regression\n",
    "(3) Ridge Regression\n",
    "  \n",
    "    \n",
    "(1) Regularization and Overfitting -    \n",
    "                                      Overfitting is an occurrence that impacts the performance of a model negatively.It occurs when a function fits \n",
    "    a limited set of data points too closely. Data often has some elements of random noise within it. For example, the training data may contain data\n",
    "    points that do not accurately represent the properties of the data. These points are considered as noise.When a function fits a set of such\n",
    "    datapoints too closely, the model learns from noise in the data. This impacts the ability of the model to make reliable predictions on test data.\n",
    "    The model may also be unable to fit additional data. This may lead to sub-par performance on test data.The image below shows the phenomena of \n",
    "    overfitting, underfitting, and the correct fit.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                       Y= β0 + β1X1 + β2X2 +…+ βpXp\n",
    "\n",
    "\n",
    "\n",
    "(2)Lasso Regression\n",
    "               Lasso regression is a regularization technique used to reduce model complexity. It is also known as L1 regularization. Lasso stands \n",
    "for Least Absolute Shrinkage and Selector Operator.\n",
    "\n",
    "\n",
    "                               ∑i=1n(yi–β0−∑j=1pβjxij)2+λ∑j=1p∣βj∣=RSS+λ∑j=1p∣βj∣\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "We note that it has a slight variation to the previously discussed loss function, with the introduction of a penalty term. To penalize highly\n",
    "fluctuating coefficients, lasso uses absolute values of the regression coefficients (∣β∣)\n",
    ".Lasso minimizes the regression coefficients to regularize the model parameters. Sometimes, Lasso can reduce regression coefficients to zero, which\n",
    "is particularly important when it comes to feature selection.\n",
    "Feature selection refers to the process of choosing relevant variables and predictors to construct a model. Here, the feature selection process is\n",
    "attributed to the ability of lasso to reduce some regression coefficients to zero. It occurs after the regression coefficients are shrunk.\n",
    "\n",
    "The predictors whose coefficients are reduced to zero will not be included in the final model. These are the predictors considered to have less \n",
    "importance. This is how some features are eliminated. However, every non-zero regression coefficient is selected for use in the model. This greatly \n",
    "assists in minimizing prediction errors.   \n",
    "    \n",
    "    \n",
    "(3) Ridge Regression-    \n",
    "                      Ridge regression refers to a type of linear regression where in order to get better predictions in the long term, we introduce\n",
    "    a small amount of bias. It is also known as L2 regularization. In ridge regression, we have the same loss function with a slight alteration in the\n",
    "    penalty term, as shown below:\n",
    "    \n",
    "\n",
    "                                      ∑i=1n(yi–β0−∑j=1pβjxij)2+λ∑j=1pβ2j=RSS+λ∑j=1pβ2j\n",
    "        \n",
    "        \n",
    "As we can see, the main difference between the above equation and the general equation for the loss function is that λ∑j=1pβ2j\n",
    " contains the squared value of the regression coefficients. The tuning parameter is λ\n",
    ". Higher values of the coefficients represent a model with greater flexibility. To penalize the flexibility of our model, we use a tuning parameter \n",
    "that decides the extent of the penalty. To minimize the function, these coefficients should be small. L2 regularization ensures the coefficients do \n",
    "not rise too high.\n",
    "\n",
    "When the value of λ\n",
    " tends to zero, the L2 regularization equation becomes the loss function of the linear regression model. The penalty term will have no effect. This\n",
    "means that if the value of λ\n",
    " is minimum, the model can be called a simple linear regression model.\n",
    "\n",
    "Conversely, when the λ\n",
    " value tends to infinity, the effect of the shrinkage penalty increases. As a result, the coefficient estimates of the ridge regression will approach\n",
    "zero. This underlines the importance of choosing a good value for λLinear or polynomial regression will likely prove unsuccessful if there is high \n",
    "collinearity between the independent variables. Ridge regression is a potential solution to handle multicollinearity.Collinearity is a condition in\n",
    "which there are two features in data that are heavily correlated to each other. Ridge regression adds an amount of bias to the regression estimates to \n",
    "reduce errors. We shall explore this in a different article.The predictors of least importance refer to predictors that bear no telling influence on \n",
    "the predictive power of the model. They may also be predictors that do not accurately describe the properties of data, such as noise.\n",
    "Ridge regression has one clear disadvantage: model interpretability. When shrinking the coefficients of the predictors of least importance, it will\n",
    "reduce them to be close to zero.        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
